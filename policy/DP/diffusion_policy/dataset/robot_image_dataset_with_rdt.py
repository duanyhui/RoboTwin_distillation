"""
ä½¿ç”¨RDTæ ‡ç­¾çš„DPæ•°æ®é›†ç±»

è¿™ä¸ªæ•°æ®é›†ä¼šåŠ è½½é¢„å…ˆè®¡ç®—å¥½çš„RDTæ¨ç†è¾“å‡ºä½œä¸ºç›‘ç£æ ‡ç­¾,è€Œä¸æ˜¯ä½¿ç”¨åŸå§‹çš„ä¸“å®¶åŠ¨ä½œ
"""

from typing import Dict
import numba
import torch
import numpy as np
import copy
from diffusion_policy.common.pytorch_util import dict_apply
from diffusion_policy.common.replay_buffer import ReplayBuffer
from diffusion_policy.common.sampler import (
    SequenceSampler,
    get_val_mask,
    downsample_mask,
)
from diffusion_policy.model.common.normalizer import LinearNormalizer
from diffusion_policy.dataset.base_dataset import BaseImageDataset
from diffusion_policy.common.normalize_util import get_image_range_normalizer
import pdb


class RobotImageDatasetWithRDT(BaseImageDataset):
    """
    ä½¿ç”¨RDTé¢„æµ‹ä½œä¸ºç›‘ç£æ ‡ç­¾çš„æ•°æ®é›†
    
    ä¸åŸå§‹RobotImageDatasetçš„åŒºåˆ«:
    1. åŠ è½½ 'rdt_action' å­—æ®µä½œä¸ºæ ‡ç­¾
    2. å¯é€‰æ‹©æ˜¯å¦ä¹Ÿä¿ç•™åŸå§‹çš„ 'action' ç”¨äºå¯¹æ¯”
    """

    def __init__(
        self,
        zarr_path,
        horizon=1,
        pad_before=0,
        pad_after=0,
        seed=42,
        val_ratio=0.0,
        batch_size=128,
        max_train_episodes=None,
        use_expert_action=False,  # ğŸ”¥ æ˜¯å¦åªç”¨ä¸“å®¶åŠ¨ä½œ(Falseåˆ™ä½¿ç”¨RDT)
        mix_expert_action=False,  # ğŸ”¥ åŒæ—¶ä½¿ç”¨ä¸“å®¶ä¸æ•™å¸ˆ, ç”Ÿæˆæ··åˆæ ‡ç­¾
        mix_alpha=0.5,            # ğŸ”¥ æ··åˆæƒé‡/æ¦‚ç‡
        mix_mode="prob",          # ğŸ”¥ prob=æŒ‰æ¦‚ç‡é€‰RDT/ä¸“å®¶; linear=çº¿æ€§åŠ æƒ
        add_expert_noise=False,   # ğŸ”¥ æ˜¯å¦å¯¹ä¸“å®¶åŠ¨ä½œåŠ é«˜æ–¯å™ªå£°
        noise_std=0.01,           # ğŸ”¥ å™ªå£°æ ‡å‡†å·®
        noise_clip=0.05,          # ğŸ”¥ å™ªå£°æˆªæ–­é˜ˆå€¼; è®¾ä¸ºNoneåˆ™ä¸æˆªæ–­
    ):

        super().__init__()
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªactionå­—æ®µ
        self.use_expert_action = use_expert_action
        self.mix_expert_action = mix_expert_action
        self.mix_alpha = mix_alpha
        self.mix_mode = mix_mode
        self.add_expert_noise = add_expert_noise
        self.noise_std = noise_std
        self.noise_clip = noise_clip

        if mix_expert_action:
            mode_tip = "æŒ‰æ¦‚ç‡é€‰æ‹© (æ··åˆæ¦‚ç‡=alpha)" if mix_mode == "prob" else "çº¿æ€§åŠ æƒ (alpha*rdt + (1-alpha)*expert)"
            print(f"ğŸ”¥ ä½¿ç”¨æ··åˆæ ‡ç­¾è¿›è¡Œè®­ç»ƒ, mix_alpha={mix_alpha}, æ¨¡å¼={mix_mode} ({mode_tip})")
            action_key = None  # ä¸‹æ–¹ä¼šç»„åˆç”Ÿæˆ
            keys = ["head_camera", "state", "action", "rdt_action"]
        else:
            action_key = 'action' if use_expert_action else 'rdt_action'
            print(f"ğŸ”¥ ä½¿ç”¨{'ä¸“å®¶åŠ¨ä½œ' if use_expert_action else 'RDTæ ‡ç­¾'}è¿›è¡Œè®­ç»ƒ")
            keys = ["head_camera", "state", action_key]

        if add_expert_noise and not (use_expert_action or mix_expert_action):
            print("âš ï¸ add_expert_noise=True ä½†æœªä½¿ç”¨ä¸“å®¶åŠ¨ä½œï¼Œæ­¤è®¾ç½®æ— æ•ˆ")
        elif add_expert_noise:
            print(f"ğŸ”Š å¯¹ä¸“å®¶åŠ¨ä½œåŠ å…¥é«˜æ–¯å™ªå£°: std={noise_std}, clip={noise_clip}")
        
        # åŠ è½½æ•°æ®
        self.replay_buffer = ReplayBuffer.copy_from_path(
            zarr_path,
            keys=keys,
        )
        
        # å…¼å®¹: å°†æœ€ç»ˆç›‘ç£æ ‡ç­¾æ”¾åˆ° self.replay_buffer['action']
        final_action = None

        def _noisify(arr):
            rng = np.random.default_rng(seed)
            noise = rng.normal(0.0, self.noise_std, size=arr.shape).astype(np.float32)
            if self.noise_clip is not None:
                noise = np.clip(noise, -self.noise_clip, self.noise_clip)
            return arr + noise

        if mix_expert_action:
            # æ··åˆæ•™å¸ˆä¸ä¸“å®¶
            expert = self.replay_buffer['action']
            if self.add_expert_noise:
                expert = _noisify(expert)
            teacher = self.replay_buffer['rdt_action']
            if self.mix_mode == "linear":
                mixed = self.mix_alpha * teacher + (1 - self.mix_alpha) * expert
            elif self.mix_mode == "prob":
                rng = np.random.default_rng(seed)
                # ä»¥mix_alphaä¸ºæ¦‚ç‡é€‰ç”¨RDTæ•´æ­¥åŠ¨ä½œï¼Œé¿å…å¤¹çˆª/å…³èŠ‚å‡ºç°åœ¨â€œåŠå¼€åŠå…³â€çš„æ— æ•ˆæ’å€¼
                mask = rng.random((expert.shape[0], 1)) < self.mix_alpha
                mixed = np.where(mask, teacher, expert)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„mix_mode: {self.mix_mode}")
            # ReplayBuffer ä¸æ”¯æŒ __setitem__, ç›´æ¥å†™ data
            final_action = mixed.astype(np.float32)
            self.replay_buffer.data['action'] = final_action
            # ä¿ç•™ teacher ä»¥ä¾¿å¯è§†åŒ–/è°ƒè¯•éœ€è¦; è‹¥æƒ³çœå†…å­˜å¯åˆ é™¤:
            # del self.replay_buffer['rdt_action']
        elif not use_expert_action:
            final_action = self.replay_buffer[action_key].astype(np.float32)
            self.replay_buffer.data['action'] = final_action
            del self.replay_buffer.data[action_key]
        else:
            # åªç”¨ä¸“å®¶åŠ¨ä½œ
            expert = self.replay_buffer['action']
            if self.add_expert_noise:
                expert = _noisify(expert)
            final_action = expert.astype(np.float32)
            self.replay_buffer.data['action'] = final_action

        val_mask = get_val_mask(n_episodes=self.replay_buffer.n_episodes, val_ratio=val_ratio, seed=seed)
        train_mask = ~val_mask
        train_mask = downsample_mask(mask=train_mask, max_n=max_train_episodes, seed=seed)

        self.sampler = SequenceSampler(
            replay_buffer=self.replay_buffer,
            sequence_length=horizon,
            pad_before=pad_before,
            pad_after=pad_after,
            episode_mask=train_mask,
        )
        self.train_mask = train_mask
        self.horizon = horizon
        self.pad_before = pad_before
        self.pad_after = pad_after

        self.batch_size = batch_size
        sequence_length = self.sampler.sequence_length
        self.buffers = {
            k: np.zeros((batch_size, sequence_length, *v.shape[1:]), dtype=v.dtype)
            for k, v in self.sampler.replay_buffer.items()
        }
        self.buffers_torch = {k: torch.from_numpy(v) for k, v in self.buffers.items()}
        for v in self.buffers_torch.values():
            v.pin_memory()

    def get_validation_dataset(self):
        val_set = copy.copy(self)
        val_set.sampler = SequenceSampler(
            replay_buffer=self.replay_buffer,
            sequence_length=self.horizon,
            pad_before=self.pad_before,
            pad_after=self.pad_after,
            episode_mask=~self.train_mask,
        )
        val_set.train_mask = ~self.train_mask
        return val_set

    def get_normalizer(self, mode="limits", **kwargs):
        data = {
            "action": self.replay_buffer["action"],
            "agent_pos": self.replay_buffer["state"],
        }
        normalizer = LinearNormalizer()
        normalizer.fit(data=data, last_n_dims=1, mode=mode, **kwargs)
        normalizer["head_cam"] = get_image_range_normalizer()
        normalizer["front_cam"] = get_image_range_normalizer()
        normalizer["left_cam"] = get_image_range_normalizer()
        normalizer["right_cam"] = get_image_range_normalizer()
        return normalizer

    def __len__(self) -> int:
        return len(self.sampler)

    def _sample_to_data(self, sample):
        agent_pos = sample["state"].astype(np.float32)
        head_cam = np.moveaxis(sample["head_camera"], -1, 1) / 255

        data = {
            "obs": {
                "head_cam": head_cam,  # T, 3, H, W
                "agent_pos": agent_pos,  # T, D
            },
            "action": sample["action"].astype(np.float32),  # T, D
        }
        return data

    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:
        if isinstance(idx, slice):
            raise NotImplementedError
        elif isinstance(idx, int):
            sample = self.sampler.sample_sequence(idx)
            sample = dict_apply(sample, torch.from_numpy)
            return sample
        elif isinstance(idx, np.ndarray):
            assert len(idx) == self.batch_size
            for k, v in self.sampler.replay_buffer.items():
                batch_sample_sequence(
                    self.buffers[k],
                    v,
                    self.sampler.indices,
                    idx,
                    self.sampler.sequence_length,
                )
            return self.buffers_torch
        else:
            raise ValueError(idx)

    def postprocess(self, samples, device):
        agent_pos = samples["state"].to(device, non_blocking=True)
        head_cam = samples["head_camera"].to(device, non_blocking=True) / 255.0
        action = samples["action"].to(device, non_blocking=True)
        return {
            "obs": {
                "head_cam": head_cam,  # B, T, 3, H, W
                "agent_pos": agent_pos,  # B, T, D
            },
            "action": action,  # B, T, D
        }


def _batch_sample_sequence(
    data: np.ndarray,
    input_arr: np.ndarray,
    indices: np.ndarray,
    idx: np.ndarray,
    sequence_length: int,
):
    for i in numba.prange(len(idx)):
        buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx = indices[idx[i]]
        data[i, sample_start_idx:sample_end_idx] = input_arr[buffer_start_idx:buffer_end_idx]
        if sample_start_idx > 0:
            data[i, :sample_start_idx] = data[i, sample_start_idx]
        if sample_end_idx < sequence_length:
            data[i, sample_end_idx:] = data[i, sample_end_idx - 1]


_batch_sample_sequence_sequential = numba.jit(_batch_sample_sequence, nopython=True, parallel=False)
_batch_sample_sequence_parallel = numba.jit(_batch_sample_sequence, nopython=True, parallel=True)


def batch_sample_sequence(
    data: np.ndarray,
    input_arr: np.ndarray,
    indices: np.ndarray,
    idx: np.ndarray,
    sequence_length: int,
):
    batch_size = len(idx)
    assert data.shape == (batch_size, sequence_length, *input_arr.shape[1:])
    if batch_size >= 16 and data.nbytes // batch_size >= 2**16:
        _batch_sample_sequence_parallel(data, input_arr, indices, idx, sequence_length)
    else:
        _batch_sample_sequence_sequential(data, input_arr, indices, idx, sequence_length)
