#!/usr/bin/env python3
"""
é¢„è®¡ç®—RDTæ¨¡å‹çš„æ¨ç†è¾“å‡ºç”¨äºæŒ‡å¯¼DPè®­ç»ƒ

åŠŸèƒ½:
1. åŠ è½½è®­ç»ƒæ•°æ®é›†(ä¸DPç›¸åŒçš„zarræ ¼å¼)
2. ä½¿ç”¨è®­ç»ƒå¥½çš„RDTæ¨¡å‹å¯¹æ¯ä¸ªæ—¶é—´æ­¥è¿›è¡Œæ¨ç†
3. ä¿å­˜RDTçš„é¢„æµ‹ç»“æœ,ç”¨äºåç»­DPè®­ç»ƒæ—¶åŠ è½½

ä½¿ç”¨æ–¹æ³•:
python precompute_rdt_labels.py \
    --rdt_ckpt checkpoints/your_model/checkpoint-10000 \
    --data_path ../DP/data/task_name-config-50.zarr \
    --output_path ./rdt_labels/task_name_labels.zarr \
    --task_name your_task \
    --instruction "your instruction text"
"""

import argparse
import os
import sys
import time
from pathlib import Path
import yaml
import zarr
import numpy as np
import torch
from tqdm import tqdm
import cv2
from PIL import Image as PImage

# æ·»åŠ RDTè·¯å¾„
current_file = Path(__file__)
parent_dir = current_file.parent
sys.path.append(str(parent_dir))

from model import RDT


def load_zarr_dataset(zarr_path):
    """åŠ è½½DPæ ¼å¼çš„zarræ•°æ®é›†"""
    print(f"åŠ è½½æ•°æ®é›†: {zarr_path}")
    
    zarr_root = zarr.open(zarr_path, mode='r')
    data = zarr_root['data']
    meta = zarr_root['meta']
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    head_camera = np.array(data['head_camera'])  # [N, C, H, W]
    state = np.array(data['state'])              # [N, D]
    action = np.array(data['action'])            # [N, D] - åŸå§‹ä¸“å®¶åŠ¨ä½œ
    episode_ends = np.array(meta['episode_ends']) # [num_episodes]
    
    print(f"  - head_camera shape: {head_camera.shape}")
    print(f"  - state shape: {state.shape}")
    print(f"  - action shape: {action.shape}")
    print(f"  - æ€»episodesæ•°: {len(episode_ends)}")
    print(f"  - æ€»æ ·æœ¬æ•°: {len(state)}")
    
    return {
        'head_camera': head_camera,
        'state': state,
        'action': action,
        'episode_ends': episode_ends
    }


def preprocess_image(img_nchw):
    """
    å°†NCHWæ ¼å¼çš„å›¾åƒè½¬æ¢ä¸ºRDTéœ€è¦çš„PILæ ¼å¼
    Args:
        img_nchw: [C, H, W] numpy array, å€¼èŒƒå›´[0, 255]
    Returns:
        PIL Image
    """
    # NCHW -> HWC
    img_hwc = np.transpose(img_nchw, (1, 2, 0))
    
    # ç¡®ä¿æ˜¯uint8æ ¼å¼
    if img_hwc.dtype != np.uint8:
        img_hwc = np.clip(img_hwc, 0, 255).astype(np.uint8)
    
    # åº”ç”¨JPEGç¼–ç /è§£ç  (ä¸RDTè®­ç»ƒæ—¶ä¸€è‡´)
    img_encoded = cv2.imencode('.jpg', img_hwc)[1].tobytes()
    img_decoded = cv2.imdecode(np.frombuffer(img_encoded, np.uint8), cv2.IMREAD_COLOR)
    
    # è½¬ä¸ºPIL Image
    return PImage.fromarray(img_decoded)


def run_rdt_inference(
    rdt_model,
    dataset,
    use_first_step_only=False,
    use_mean_steps=None,
    instruction=None,
    stationary_mask_eps=None,
):
    """
    å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬è¿è¡ŒRDTæ¨ç†
    
    Args:
        rdt_model: å·²åŠ è½½çš„RDTæ¨¡å‹
        dataset: æ•°æ®é›†å­—å…¸
        use_first_step_only: æ˜¯å¦åªä½¿ç”¨RDTé¢„æµ‹çš„ç¬¬ä¸€æ­¥ (é»˜è®¤True)
        use_mean_steps: å¦‚æœä¸ä¸ºNone,ä½¿ç”¨å‰Næ­¥çš„å¹³å‡å€¼ä½œä¸ºæ ‡ç­¾
    
    Returns:
        rdt_predictions: [N, action_dim] numpy array
    """
    head_camera = dataset['head_camera']
    state = dataset['state']
    expert_action = dataset['action']
    episode_ends = dataset['episode_ends']
    instruction_text = instruction if instruction else rdt_model.task_name
    
    num_samples = len(state)
    data_action_dim = dataset['action'].shape[1]  # æ•°æ®é›†çš„åŸå§‹åŠ¨ä½œç»´åº¦ (ä¾‹å¦‚14)
    
    # åˆå§‹åŒ–è¾“å‡ºæ•°ç»„ - ä½¿ç”¨æ•°æ®é›†çš„åŸå§‹ç»´åº¦
    rdt_predictions = np.zeros((num_samples, data_action_dim), dtype=np.float32)
    
    # è·å–RDTæ¨¡å‹çš„åŠ¨ä½œç»´åº¦
    model_action_dim = rdt_model.left_arm_dim + 1 + rdt_model.right_arm_dim + 1
    
    print(f"\nå¼€å§‹RDTæ¨ç†...")
    print(f"  - æ•°æ®é›†åŠ¨ä½œç»´åº¦: {data_action_dim}")
    print(f"  - RDTæ¨¡å‹åŠ¨ä½œç»´åº¦: {model_action_dim}")
    print(f"  - æ–‡æœ¬æŒ‡ä»¤: {instruction_text}")
    if stationary_mask_eps is not None:
        print(f"  - é™æ­¢æ©ç : é€Ÿåº¦é˜ˆå€¼ eps={stationary_mask_eps}")
    
    if data_action_dim != model_action_dim:
        print(f"  âš ï¸  åŠ¨ä½œç»´åº¦ä¸åŒ¹é…! å°†è‡ªåŠ¨è½¬æ¢ {model_action_dim}ç»´ â†’ {data_action_dim}ç»´")
    
    # è·å–episodeçš„èµ·å§‹å’Œç»“æŸç´¢å¼•
    episode_starts = [0] + episode_ends[:-1].tolist()
    episode_ends_list = episode_ends.tolist()
    
    if use_first_step_only:
        strategy_desc = "ä»…ä½¿ç”¨ç¬¬1æ­¥"
    elif use_mean_steps is not None:
        strategy_desc = f"ä½¿ç”¨å‰{use_mean_steps}æ­¥å¹³å‡"
    else:
        strategy_desc = "æœªå¯ç”¨å¹³æ»‘ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬1æ­¥"
    print(f"  - é…ç½®: {strategy_desc}")
    
    sample_idx = 0
    
    # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
    data_state_dim = state.shape[1]  # æ•°æ®é›†çš„çŠ¶æ€ç»´åº¦
    model_state_dim = rdt_model.left_arm_dim + 1 + rdt_model.right_arm_dim + 1  # æ¨¡å‹æœŸæœ›çš„ç»´åº¦
    
    if data_state_dim != model_state_dim:
        print(f"\nâš ï¸  è­¦å‘Š: ç»´åº¦ä¸åŒ¹é…!")
        print(f"  - æ•°æ®é›†çŠ¶æ€ç»´åº¦: {data_state_dim}")
        print(f"  - æ¨¡å‹æœŸæœ›ç»´åº¦: {model_state_dim} (left_arm={rdt_model.left_arm_dim}, right_arm={rdt_model.right_arm_dim})")
        print(f"  - æ•°æ®é›†æ ¼å¼å‡è®¾: [left_arm(7), left_gripper(1), right_arm(6), right_gripper(1)]")
        
        if data_state_dim < model_state_dim:
            print(f"  - è‡ªåŠ¨å¡«å……åˆ° {model_state_dim} ç»´...")
            
            # å‡è®¾æ•°æ®æ ¼å¼: [left_arm(7), left_gripper(1), right_arm(6), right_gripper(1)] = 15ç»´
            # æˆ–è€…: [left_arm(7), left_gripper(1), right_arm(5), right_gripper(1)] = 14ç»´
            # æ¨¡å‹æœŸæœ›: [left_arm(8), left_gripper(1), right_arm(8), right_gripper(1)] = 18ç»´
            # æˆ–: [left_arm(8), left_gripper(1), right_arm(7), right_gripper(1)] = 17ç»´
            
            state_padded = np.zeros((num_samples, model_state_dim), dtype=np.float32)
            
            # è®¡ç®—å®é™…çš„armç»´åº¦ (æ•°æ®é›†ä¸­)
            data_left_arm_dim = (data_state_dim - 2) // 2  # å‡å»2ä¸ªgripper,ç„¶åå¹³åˆ†
            data_right_arm_dim = data_state_dim - data_left_arm_dim - 2
            
            print(f"  - æ£€æµ‹åˆ°æ•°æ®é›†: left_arm={data_left_arm_dim}, right_arm={data_right_arm_dim}")
            
            # å¤åˆ¶å·¦è‡‚å…³èŠ‚ (å°½å¯èƒ½å¤šåœ°å¤åˆ¶)
            left_copy_dim = min(data_left_arm_dim, rdt_model.left_arm_dim)
            state_padded[:, :left_copy_dim] = state[:, :left_copy_dim]
            # å¦‚æœéœ€è¦å¡«å……,å‰©ä½™ç»´åº¦ä¿æŒä¸º0
            
            # å¤åˆ¶å·¦å¤¹çˆª
            state_padded[:, rdt_model.left_arm_dim] = state[:, data_left_arm_dim]
            
            # å¤åˆ¶å³è‡‚å…³èŠ‚
            right_copy_dim = min(data_right_arm_dim, rdt_model.right_arm_dim)
            state_padded[:, rdt_model.left_arm_dim+1:rdt_model.left_arm_dim+1+right_copy_dim] = \
                state[:, data_left_arm_dim+1:data_left_arm_dim+1+right_copy_dim]
            
            # å¤åˆ¶å³å¤¹çˆª
            state_padded[:, rdt_model.left_arm_dim+1+rdt_model.right_arm_dim] = state[:, data_state_dim-1]
            
            state = state_padded
            print(f"  âœ… å¡«å……å®Œæˆ! æ–°ç»´åº¦: {state.shape}")
        else:
            raise ValueError(f"æ•°æ®ç»´åº¦({data_state_dim})å¤§äºæ¨¡å‹æœŸæœ›({model_state_dim}), æ— æ³•è‡ªåŠ¨å¤„ç†")
    
    for ep_idx, (ep_start, ep_end) in enumerate(zip(episode_starts, episode_ends_list)):
        print(f"\nå¤„ç† Episode {ep_idx + 1}/{len(episode_ends_list)} (æ ·æœ¬ {ep_start} - {ep_end})")
        
        # æ¯ä¸ªepisodeå¼€å§‹æ—¶é‡ç½®æ¨¡å‹
        rdt_model.reset_obsrvationwindows()
        rdt_model.set_language_instruction(instruction_text)  # ä½¿ç”¨æŒ‡å®šæŒ‡ä»¤
   
        # åˆå§‹åŒ–è§‚å¯Ÿçª—å£ (éœ€è¦ä¸¤å¸§)
        # ç¬¬ä¸€å¸§ç”¨dummy
        rdt_model.observation_window = None
        
        for t in tqdm(range(ep_start, ep_end), desc=f"Episode {ep_idx + 1}"):
            # å‡†å¤‡å½“å‰å¸§çš„å›¾åƒ (NCHW -> HWC -> BGR uint8)
            current_img_nchw = head_camera[t]  # [3, H, W]
            current_img_hwc = np.transpose(current_img_nchw, (1, 2, 0))  # [H, W, 3]
            
            # ç¡®ä¿æ˜¯uint8 BGRæ ¼å¼
            if current_img_hwc.dtype != np.uint8:
                current_img_hwc = np.clip(current_img_hwc, 0, 255).astype(np.uint8)
            # è®­ç»ƒæœŸä½¿ç”¨ RGBï¼Œè¿™é‡Œä» BGR è½¬å› RGB ä»¥å‡å°é¢œè‰²åŸŸåç§»
            current_img_hwc = cv2.cvtColor(current_img_hwc, cv2.COLOR_BGR2RGB)
            
            # cv2.imshow("RDT", current_img_hwc)
            # cv2.waitKey(1)
            
            # å‡†å¤‡å‰ä¸€å¸§çš„å›¾åƒ
            if t == ep_start:
                # ç¬¬ä¸€å¸§: ä½¿ç”¨å½“å‰å¸§ä½œä¸ºå‰ä¸€å¸§
                prev_img_hwc = current_img_hwc.copy()
            else:
                prev_img_nchw = head_camera[t-1]
                prev_img_hwc = np.transpose(prev_img_nchw, (1, 2, 0))
                if prev_img_hwc.dtype != np.uint8:
                    prev_img_hwc = np.clip(prev_img_hwc, 0, 255).astype(np.uint8)
                prev_img_hwc = cv2.cvtColor(prev_img_hwc, cv2.COLOR_BGR2RGB)
            
            # å‡†å¤‡å›¾åƒæ•°ç»„ (RDTçš„update_observation_windowéœ€è¦3ä¸ªç›¸æœºçš„å›¾åƒ)
            # ä½†æˆ‘ä»¬åªæœ‰head_cameraï¼Œæ‰€ä»¥å¤åˆ¶å®ƒæ¥å¡«å……3ä¸ªä½ç½®
            img_arr = [
                current_img_hwc,  # head/front camera
                current_img_hwc,  # right camera (ç”¨headæ›¿ä»£)
                current_img_hwc,  # left camera (ç”¨headæ›¿ä»£)
            ]
            
            # æ›´æ–°è§‚å¯Ÿçª—å£ (è¿™ä¼šè‡ªåŠ¨å¤„ç†ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶çš„åˆå§‹åŒ–)
            rdt_model.update_observation_window(img_arr, state[t])
            
            # è·å–RDTåŠ¨ä½œé¢„æµ‹
            # actions shape: [64, model_action_dim]
            actions = rdt_model.get_action()
            
            # æå–ç›‘ç£æ ‡ç­¾
            if use_first_step_only:
                # æ–¹æ¡ˆ1: åªä½¿ç”¨ç¬¬ä¸€æ­¥
                predicted_action = actions[0]
            elif use_mean_steps is not None:
                # æ–¹æ¡ˆ2: ä½¿ç”¨å‰Næ­¥çš„å¹³å‡
                predicted_action = actions[:use_mean_steps].mean(axis=0)
            else:
                # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€æ­¥
                predicted_action = actions[0]
            
            # è½¬æ¢åŠ¨ä½œç»´åº¦ (å¦‚æœéœ€è¦)
            if model_action_dim != data_action_dim:
                # å°†RDTçš„é¢„æµ‹è½¬æ¢ä¸ºæ•°æ®é›†çš„æ ¼å¼
                # å‡è®¾æ ¼å¼: [left_arm, left_gripper, right_arm, right_gripper]
                
                # è®¡ç®—æ•°æ®é›†çš„armç»´åº¦
                data_left_arm = (data_action_dim - 2) // 2
                data_right_arm = data_action_dim - data_left_arm - 2
                
                converted_action = np.zeros(data_action_dim, dtype=np.float32)
                
                # å¤åˆ¶å·¦è‡‚ (å–å‰data_left_armç»´)
                converted_action[:data_left_arm] = predicted_action[:data_left_arm]
                
                # å¤åˆ¶å·¦å¤¹çˆª
                converted_action[data_left_arm] = predicted_action[rdt_model.left_arm_dim]
                
                # å¤åˆ¶å³è‡‚
                converted_action[data_left_arm+1:data_left_arm+1+data_right_arm] = \
                    predicted_action[rdt_model.left_arm_dim+1:rdt_model.left_arm_dim+1+data_right_arm]
                
                # å¤åˆ¶å³å¤¹çˆª
                converted_action[-1] = predicted_action[model_action_dim-1]
                
                rdt_predictions[t] = converted_action
            else:
                # ç»´åº¦åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
                rdt_predictions[t] = predicted_action
            
            # æ–¹æ¡ˆ1: é™æ­¢çŠ¶æ€æ©ç  (ä¸“å®¶ä¸åŠ¨æ—¶å¼ºåˆ¶å¯¹é½ä¸“å®¶ï¼ŒæŠ‘åˆ¶RDTæ¼‚ç§»/æŠ¢è·‘)
            if stationary_mask_eps is not None:
                prev_idx = max(ep_start, t - 1)
                expert_curr = expert_action[t]
                expert_prev = expert_action[prev_idx]
                vel = np.abs(expert_curr - expert_prev)
                mask = vel < stationary_mask_eps  # ä¸“å®¶ç»´æŒé™æ­¢çš„ç»´åº¦
                if np.any(mask):
                    # å°†è¿™äº›ç»´åº¦çš„æ ‡ç­¾æ›¿æ¢æˆä¸“å®¶çš„åŠ¨ä½œ
                    rdt_predictions[t, mask] = expert_curr[mask]
            
            sample_idx += 1
    
    print(f"\nâœ… RDTæ¨ç†å®Œæˆ! å…±å¤„ç† {num_samples} ä¸ªæ ·æœ¬")
    return rdt_predictions


def save_rdt_labels(output_path, dataset, rdt_predictions):
    """
    ä¿å­˜RDTé¢„æµ‹çš„æ ‡ç­¾
    ä¿æŒä¸åŸå§‹æ•°æ®é›†ç›¸åŒçš„æ ¼å¼,ä½†æ·»åŠ rdt_actionå­—æ®µ
    """
    print(f"\nä¿å­˜RDTæ ‡ç­¾åˆ°: {output_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶
    if os.path.exists(output_path):
        import shutil
        shutil.rmtree(output_path)
    
    # åˆ›å»ºæ–°çš„zarræ–‡ä»¶
    zarr_root = zarr.group(output_path)
    zarr_data = zarr_root.create_group('data')
    zarr_meta = zarr_root.create_group('meta')
    
    compressor = zarr.Blosc(cname='zstd', clevel=3, shuffle=1)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    zarr_data.create_dataset(
        'head_camera',
        data=dataset['head_camera'],
        chunks=(100, *dataset['head_camera'].shape[1:]),
        overwrite=True,
        compressor=compressor,
    )
    
    zarr_data.create_dataset(
        'state',
        data=dataset['state'],
        chunks=(100, dataset['state'].shape[1]),
        dtype='float32',
        overwrite=True,
        compressor=compressor,
    )
    
    # ä¿å­˜åŸå§‹ä¸“å®¶åŠ¨ä½œ
    zarr_data.create_dataset(
        'action',
        data=dataset['action'],
        chunks=(100, dataset['action'].shape[1]),
        dtype='float32',
        overwrite=True,
        compressor=compressor,
    )
    
    # ğŸ”¥ ä¿å­˜RDTé¢„æµ‹çš„åŠ¨ä½œ (è¿™æ˜¯æ–°å¢çš„!)
    zarr_data.create_dataset(
        'rdt_action',
        data=rdt_predictions,
        chunks=(100, rdt_predictions.shape[1]),
        dtype='float32',
        overwrite=True,
        compressor=compressor,
    )
    
    zarr_meta.create_dataset(
        'episode_ends',
        data=dataset['episode_ends'],
        dtype='int64',
        overwrite=True,
        compressor=compressor,
    )
    
    print(f"âœ… ä¿å­˜å®Œæˆ!")
    print(f"  - åŒ…å«å­—æ®µ: head_camera, state, action, rdt_action, episode_ends")


def main():
    parser = argparse.ArgumentParser(description='é¢„è®¡ç®—RDTæ¨ç†è¾“å‡ºç”¨äºDPè®­ç»ƒ')
    
    # RDTæ¨¡å‹å‚æ•°
    parser.add_argument('--rdt_ckpt', type=str, required=True,
                        help='RDT checkpointè·¯å¾„')
    parser.add_argument('--task_name', type=str, required=True,
                        help='ä»»åŠ¡åç§°')
    parser.add_argument('--instruction', type=str, default=None,
                        help='è¯­è¨€æŒ‡ä»¤ (å¯é€‰,é»˜è®¤ä½¿ç”¨task_name)')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, required=True,
                        help='è¾“å…¥æ•°æ®é›†è·¯å¾„ (DPæ ¼å¼çš„zarr)')
    parser.add_argument('--output_path', type=str, required=True,
                        help='è¾“å‡ºæ ‡ç­¾è·¯å¾„')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--left_arm_dim', type=int, default=-1,
                        help='å·¦è‡‚ç»´åº¦ï¼Œ-1 æ—¶è‡ªåŠ¨æ ¹æ®æ•°æ®ç»´åº¦æ¨æ–­')
    parser.add_argument('--right_arm_dim', type=int, default=-1,
                        help='å³è‡‚ç»´åº¦ï¼Œ-1 æ—¶è‡ªåŠ¨æ ¹æ®æ•°æ®ç»´åº¦æ¨æ–­')
    parser.add_argument('--rdt_step', type=int, default=64,
                        help='RDT chunk size')
    
    # æ ‡ç­¾æå–ç­–ç•¥
    parser.add_argument('--use_first_step', action='store_true', default=False,
                        help='åªä½¿ç”¨RDTé¢„æµ‹çš„ç¬¬1æ­¥ä½œä¸ºæ ‡ç­¾ (é»˜è®¤å…³é—­ï¼Œå¯ç”¨ --use_mean_steps åšå¹³æ»‘)')
    parser.add_argument('--use_mean_steps', type=int, default=None,
                        help='ä½¿ç”¨å‰Næ­¥çš„å¹³å‡ä½œä¸ºæ ‡ç­¾')
    parser.add_argument('--stationary_mask_eps', type=float, default=None,
                        help='é™æ­¢æ©ç é˜ˆå€¼; è‹¥ä¸“å®¶é€Ÿåº¦<epsåˆ™ç”¨ä¸“å®¶åŠ¨ä½œè¦†ç›– (æ¨è0.01~0.02)')
    
    args = parser.parse_args()
    
    # 1. åŠ è½½æ•°æ®é›†
    dataset = load_zarr_dataset(args.data_path)

    # è‡ªåŠ¨æ¨æ–­è‡‚ç»´åº¦
    data_dim = dataset["state"].shape[1]
    inferred_left = (data_dim - 2) // 2
    inferred_right = data_dim - inferred_left - 2
    if args.left_arm_dim < 0 or args.right_arm_dim < 0:
        args.left_arm_dim = inferred_left
        args.right_arm_dim = inferred_right
        print(f"  - è‡ªåŠ¨æ¨æ–­å…³èŠ‚ç»´åº¦: left_arm={args.left_arm_dim}, right_arm={args.right_arm_dim}")
    else:
        if args.left_arm_dim + args.right_arm_dim + 2 != data_dim:
            print(f"âš ï¸ æŒ‡å®šçš„è‡‚ç»´åº¦ ({args.left_arm_dim}+{args.right_arm_dim}+2) ä¸æ•°æ®ç»´åº¦ {data_dim} ä¸ä¸€è‡´ï¼Œè¯·ç¡®è®¤ï¼")

    # å¤„ç†æ ‡ç­¾ç­–ç•¥å†²çª
    if args.use_first_step and args.use_mean_steps is not None:
        print("âš ï¸ åŒæ—¶æŒ‡å®šäº† --use_first_step å’Œ --use_mean_stepsï¼Œä¼˜å…ˆä½¿ç”¨å‡å€¼å¹³æ»‘ã€‚")
        args.use_first_step = False
    
    # 2. åˆå§‹åŒ–RDTæ¨¡å‹
    print(f"\nåˆå§‹åŒ–RDTæ¨¡å‹...")
    print(f"  - Checkpoint: {args.rdt_ckpt}")
    print(f"  - Task: {args.task_name}")
    
    # æ£€æµ‹checkpointæ ¼å¼å¹¶é€‰æ‹©æ­£ç¡®çš„è·¯å¾„
    ckpt_dir = args.rdt_ckpt
    ds_checkpoint = os.path.join(ckpt_dir, "pytorch_model", "mp_rank_00_model_states.pt")
    ema_checkpoint = os.path.join(ckpt_dir, "ema", "model.safetensors")
    
    if os.path.isfile(ds_checkpoint):
        pretrained_path = ds_checkpoint
        print(f"  - ä½¿ç”¨DeepSpeed checkpoint: {ds_checkpoint}")
    elif any(os.path.isfile(os.path.join(ckpt_dir, fname)) for fname in ("model.safetensors", "pytorch_model.bin")):
        pretrained_path = ckpt_dir  # HuggingFace style checkpoint folder
        print(f"  - ä½¿ç”¨HuggingFace checkpoint: {ckpt_dir}")
    elif os.path.isfile(ema_checkpoint):
        pretrained_path = ema_checkpoint
        print(f"  - ä½¿ç”¨EMA checkpoint: {ema_checkpoint}")
    else:
        raise FileNotFoundError(f"âŒ æ— æ³•åœ¨ {ckpt_dir} ä¸‹æ‰¾åˆ°å¯ç”¨çš„RDTæƒé‡æ–‡ä»¶")
    
    rdt = RDT(
        pretrained_path,
        args.task_name,
        args.left_arm_dim,
        args.right_arm_dim,
        args.rdt_step
    )
    
    # è®¾ç½®æŒ‡ä»¤
    instruction = args.instruction if args.instruction else args.task_name
    rdt.set_language_instruction(instruction)
    
    print(f"âœ… RDTæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 3. è¿è¡Œæ¨ç†
    rdt_predictions = run_rdt_inference(
        rdt,
        dataset,
        use_first_step_only=args.use_first_step,
        use_mean_steps=args.use_mean_steps,
        instruction=instruction,
        stationary_mask_eps=args.stationary_mask_eps,
    )
    
    # 4. ä¿å­˜ç»“æœ
    save_rdt_labels(args.output_path, dataset, rdt_predictions)
    
    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆ!")
    print(f"{'='*60}")
    print(f"ç°åœ¨æ‚¨å¯ä»¥ä¿®æ”¹DPçš„datasetä»£ç ,è¯»å– 'rdt_action' ä½œä¸ºç›‘ç£æ ‡ç­¾")


if __name__ == '__main__':
    main()
